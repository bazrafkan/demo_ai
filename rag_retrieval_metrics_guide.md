# What is K in Retrieval? Why Do We Use It?

In retrieval-based systems (like RAG), **K** represents the number of top retrieved results considered for ranking, classification, or further processing.

---

## ğŸ”¹ Why is K Important?

When retrieving documents from a knowledge base or vector store, we usually donâ€™t retrieve just one resultâ€”we retrieve the top **K** results that are the most relevant based on similarity.

- âœ… **Ensures Relevant Documents Are Retrieved** â€“ If we only retrieve the top 1 result (K=1), we risk missing important context.
- âœ… **Improves Accuracy** â€“ Larger K increases the chance that the relevant document is included.
- âœ… **Balances Speed vs. Quality** â€“ A very high K slows down processing, but a small K might not retrieve enough useful data.

---

## ğŸ”¹ Examples of K in Different Metrics

### 1ï¸âƒ£ Recall@K

- **Definition**: Measures whether the correct answer is within the top K retrieved results.
- **Behavior**: Higher K improves recall but may increase noise.

**Example**:  
You retrieve K=3 documents for a legal classification problem.  

- If the correct document is among them â†’ Recall@3 = 1  
- If not â†’ Recall@3 = 0  

ğŸš€ **Interpretation**: Higher K improves recall, but too high can introduce irrelevant results.

---

### 2ï¸âƒ£ MRR (Mean Reciprocal Rank)

- **Definition**: Checks how early the correct document appears in the top K results.
- **Behavior**:
  - If the correct document is ranked 1st, MRR is 1.0.  
  - If ranked 3rd, MRR is 1/3 = 0.33.

**Example (K=5 results ranked by relevance)**:  
1ï¸âƒ£ Correct Answer â€“ MRR = 1/1 = 1.0  
2ï¸âƒ£ Wrong Answer  
3ï¸âƒ£ Wrong Answer  
4ï¸âƒ£ Correct Answer â€“ MRR = 1/4 = 0.25  
5ï¸âƒ£ Wrong Answer  

ğŸš€ **Interpretation**: We want relevant documents to appear earlier (higher rank).

---

### 3ï¸âƒ£ Precision@K

- **Definition**: Measures how many of the top K retrieved results are actually relevant.
- **Example**:
  - If K=5, and 3 out of 5 documents are correct, then Precision@5 = 3/5 = 0.6.

ğŸš€ **Interpretation**: Helps tune K to avoid retrieving too many irrelevant documents.

---

## ğŸ”¹ Choosing the Right K

| **K Value** | **Use Case** |
|-------------|--------------|
| K = 1       | Best for high-precision tasks (e.g., yes/no classification). |
| K = 3-5     | Balanced for most RAG applications (ensures good recall). |
| K = 10+     | Needed for broad knowledge retrieval (e.g., research papers, legal case law). |

- ğŸ”¹ **Lower K** â†’ Faster, but may miss relevant knowledge.  
- ğŸ”¹ **Higher K** â†’ More accurate but slower & may include irrelevant noise.

---

## ğŸ”¹ Final Thought

- In RAG, we use **K** to balance speed vs. accuracy.
- **K** should be tuned based on experiments (e.g., track Recall@K and MRR).
- The ideal **K** depends on your retrieval method and domain.

---

## âœ… Recall@K, Precision@K, MRR@K â€“ Definitions & Ranges

### ğŸ”¹ Recall@K

- **Definition**: Measures whether the correct item is found within the top K results.
- **Range**:  
    `0 â‰¤ Recall@K â‰¤ 1`  
- **Interpretation**:  
  - `1.0` â†’ Correct item is in the top K results.  
  - `0.0` â†’ Correct item is not in top K.  

âœ… **Best for**: Comprehensive retrieval â€“ ensuring relevant docs arenâ€™t missed.

---

### ğŸ”¹ Precision@K

- **Definition**: Measures how many of the top K results are actually relevant.
- **Range**:  
    `0 â‰¤ Precision@K â‰¤ 1`  
- **Formula**:  
    `Precision@K = (# of relevant results in top K) / K`  
- **Interpretation**:  
  - `1.0` â†’ All top K results are relevant.  
  - `0.0` â†’ None of the top K results are relevant.  

âœ… **Best for**: Reducing irrelevant documents â€“ good when K is small.

---

### ğŸ”¹ MRR@K (Mean Reciprocal Rank)

- **Definition**: Measures how early in the top K list the first relevant result appears.
- **Range**:  
    `0 < MRR@K â‰¤ 1`  
- **Formula**:  
    `MRR@K = 1 / rank_of_first_relevant_result`  
- **Interpretation**:
  - `1.0` â†’ Correct answer is ranked 1st.
  - `0.5` â†’ Correct answer is ranked 2nd, etc.
  - If not in top K, MRR is 0.  

âœ… **Best for**: Prioritizing early correctness â€“ useful when users look at the top result only.

---

## ğŸ“Š Metric Comparison Example (K = 5)

| **Retrieved Docs** | **Correct Label** | **Recall@5** | **Precision@5** | **MRR@5** |
|---------------------|-------------------|--------------|------------------|-----------|
| [A, B, C, D, E]     | B                 | 1            | 1/5 = 0.2        | 1/2 = 0.5 |
| [F, G, H, I, J]     | K                 | 0            | 0                | 0         |
| [L, M, N, O, P]     | L                 | 1            | 1/5 = 0.2        | 1         |

---

## ğŸ§  How to Use These Ranges

- If **Recall@K** is low, increase K or improve retrieval.
- If **Precision@K** is low, your model is returning too many irrelevant results.
- If **MRR** is low, relevant items are buried deep â€“ use reranking!
